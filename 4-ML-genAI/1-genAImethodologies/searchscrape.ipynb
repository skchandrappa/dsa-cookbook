{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.31.0 onnx==1.14.0 onnxruntime==1.15.1 optimum==1.12.0"
      ],
      "metadata": {
        "id": "MIhUA2xb2HUo",
        "outputId": "99b82065-7d0a-4ba0-b8cc-0bb3460a991e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.31.0\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx==1.14.0\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting onnxruntime==1.15.1\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting optimum==1.12.0\n",
            "  Downloading optimum-1.12.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.67.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx==1.14.0) (4.25.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx==1.14.0) (4.12.2)\n",
            "Collecting coloredlogs (from onnxruntime==1.15.1)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.15.1) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.15.1) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from optimum==1.12.0) (2.5.1+cu121)\n",
            "Collecting datasets (from optimum==1.12.0)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2024.10.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum==1.12.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum==1.12.0) (3.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime==1.15.1) (1.3.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.12.0) (0.2.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.15.1)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.12.0) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->optimum==1.12.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.12.0) (2.2.2)\n",
            "Collecting xxhash (from datasets->optimum==1.12.0)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->optimum==1.12.0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum==1.12.0) (3.11.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2024.12.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.0) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.0) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.0) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum==1.12.0) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->optimum==1.12.0) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum==1.12.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum==1.12.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum==1.12.0) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum==1.12.0) (1.17.0)\n",
            "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum-1.12.0-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.6/380.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, xxhash, onnx, humanfriendly, fsspec, dill, multiprocess, coloredlogs, transformers, onnxruntime, datasets, optimum\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.47.0\n",
            "    Uninstalling transformers-4.47.0:\n",
            "      Successfully uninstalled transformers-4.47.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed coloredlogs-15.0.1 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 humanfriendly-10.0 multiprocess-0.70.16 onnx-1.14.0 onnxruntime-1.15.1 optimum-1.12.0 tokenizers-0.13.3 transformers-4.31.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "import onnx\n",
        "\n",
        "# 1. Download the model\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# 2. Convert the model to ONNX\n",
        "# Adjust input shape to have only batch size and sequence length\n",
        "dummy_input = torch.randint(0, 30522, (1, 16)).to(torch.int64) # Assume vocab size of 30522, adjust as needed\n",
        "# Use input_ids and attention_mask as inputs for the export\n",
        "torch.onnx.export(model, (dummy_input, torch.ones_like(dummy_input)), \"bert-base-uncased.onnx\",\n",
        "                  input_names=['input_ids', 'attention_mask']) # Specify input names\n",
        "\n"
      ],
      "metadata": {
        "id": "5BNYMqiT1_jZ",
        "outputId": "c6f3801c-bb1e-4cc7-d312-d950ccea0748",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load the ONNX model and set up inference\n",
        "import onnxruntime as ort\n",
        "\n",
        "# Create an inference session\n",
        "session = ort.InferenceSession(\"bert-base-uncased.onnx\")\n",
        "\n",
        "# # Prepare input data\n",
        "# input_ids = torch.tensor([[101, 2057, 2049, 102]]).to(torch.int64)\n",
        "# attention_mask = torch.tensor([[1, 1, 1, 1]]).to(torch.int64)\n",
        "\n",
        "# Prepare input data\n",
        "# Adjust the input sequence length to match the expected length (16)\n",
        "input_ids = torch.tensor([[101, 2057, 2049, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]).to(torch.int64)\n",
        "attention_mask = torch.tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]).to(torch.int64)\n",
        "\n",
        "\n",
        "# Run the model\n",
        "input_names = [input.name for input in session.get_inputs()]\n",
        "output_names = [output.name for output in session.get_outputs()]\n",
        "\n",
        "outputs = session.run(output_names, {input_names[0]: input_ids.numpy(), input_names[1]: attention_mask.numpy()})\n",
        "\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "0zivU-uX4gCF",
        "outputId": "a268aef6-1cfa-483f-fef0-d2bddd3aa322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[-0.02504878, -0.02753285]], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #### Pre Installs\n",
        "# import os\n",
        "# os.environ['openAIAPIKey'] = ''\n",
        "# os.environ['serpAPIKey'] = ''\n",
        "!pip install google-search-results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhApqbbXAupk",
        "outputId": "1575afa0-8ac2-4f27-b5e9-9d20efe5a045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "appConfig = {\n",
        "    \"NoOfLinks\" : 3,\n",
        "    \"oaSystemprompt\" : \"You are an assistant that analyzes the contents of a website and extracts requested details in a json format with provided keys and ignoring and other text. Respond in markdown.\",\n",
        "    \"oaUserprompt\" : \"\"\n",
        "}"
      ],
      "metadata": {
        "id": "3Jro5ntlS68W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "class getApiKeys:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def getApiKeys(self,apiKey):\n",
        "    return os.environ.get(apiKey)"
      ],
      "metadata": {
        "id": "OKQztJvr9_6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import serpapi\n",
        "import json\n",
        "from serpapi import GoogleSearch # Add this import statement\n",
        "\n",
        "class searchForLinks:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def getSearchResults(self,searchWord):\n",
        "    searpApiKey = getApiKeys()\n",
        "    params = {\n",
        "      \"q\": searchWord,\n",
        "      \"hl\": \"en\",\n",
        "      \"gl\": \"us\",\n",
        "      \"google_domain\": \"google.com\",\n",
        "      \"api_key\": ''\n",
        "    }\n",
        "\n",
        "    search = GoogleSearch(params)\n",
        "    results = search.get_dict()\n",
        "    linksList = []\n",
        "\n",
        "    if 'organic_results' in results:\n",
        "        for res in results['organic_results']:\n",
        "            if 'link' in res:  # Check if 'link' key exists in each result dictionary\n",
        "                linksList.append(res['link'])\n",
        "    else:\n",
        "        print(\"No organic results found in the response.\")\n",
        "\n",
        "    return linksList[:appConfig['NoOfLinks']]"
      ],
      "metadata": {
        "id": "B9I34FupATv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dBhuzSauPlr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "class webCrawler:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def crawlresults(self,srch):\n",
        "    sfl = searchForLinks()\n",
        "    linktocrawl = sfl.getSearchResults(srch)\n",
        "\n",
        "    crawlJson = {}\n",
        "    for links in linktocrawl:\n",
        "      crawlJson.update(self.crawl(links))\n",
        "\n",
        "    return crawlJson\n",
        "\n",
        "  def crawl(self, url: str):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    title = soup.title.string if soup.title else \"Title not found\"\n",
        "\n",
        "    if soup.body:\n",
        "      for irrelevant in soup.body([\"script\", \"style\",\"img\",\"input\"]):\n",
        "        irrelevant.decompose()\n",
        "      text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
        "    else:\n",
        "      text = \"\"\n",
        "\n",
        "    return {title : text}\n",
        "\n",
        "    # links = [link.get('href') for link in soup.find_all('a')]\n",
        "    # self.links = [link for link in links if link]"
      ],
      "metadata": {
        "id": "nRr7KRn8AeY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "# from dotenv import load_dotenv\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import Markdown, display\n",
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "\n",
        "class summarizer:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def user_prompt_for(self,title,text):\n",
        "      user_prompt = f\"You are looking at a website titled {title}\"\n",
        "      user_prompt += \"The contents of this website is as follows; please find any value about APY, Terms rewards and respond as as JSON with following format {'apy':'','fees':''}. ignore anything else.\\n\\n\"\n",
        "      user_prompt += text\n",
        "      return user_prompt\n",
        "\n",
        "  def messages_for(self,content):\n",
        "      return [\n",
        "          {\"role\": \"system\", \"content\": appConfig['oaSystemprompt']},\n",
        "          {\"role\": \"user\", \"content\": content}\n",
        "      ]\n",
        "\n",
        "  def summarize(self,url):\n",
        "      searpApiKey = getApiKeys()\n",
        "      api_key = searpApiKey.getApiKeys('openAIAPIKey')\n",
        "\n",
        "      openai = OpenAI(api_key=api_key)\n",
        "\n",
        "      crawl = webCrawler()\n",
        "      contents = crawl.crawlresults(url)\n",
        "\n",
        "      responses = []\n",
        "\n",
        "      for content in contents:\n",
        "        user_prompt = self.user_prompt_for(content,contents[content])\n",
        "        messages = self.messages_for(content)\n",
        "\n",
        "        response = openai.chat.completions.create(\n",
        "            model = \"gpt-4o-mini\",\n",
        "            messages = messages\n",
        "        )\n",
        "        responses.append(response.choices[0].message.content)\n",
        "\n",
        "      return responses\n"
      ],
      "metadata": {
        "id": "R2Zx1YHBUb86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract = summarizer()\n",
        "results = extract.summarize('chase freedom credit card')\n",
        "for I in results:\n",
        "  display(Markdown(I))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "PKUO9wpKAkgK",
        "outputId": "cae85573-fa99-4e6b-8d7e-c9230494efa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```json\n{\n  \"title\": \"Chase Freedom\",\n  \"category\": \"Credit Cards\",\n  \"website\": \"Chase.com\"\n}\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```json\n{\n  \"card_name\": \"Chase Freedom Unlimited\",\n  \"issuer\": \"Chase\",\n  \"rewards\": {\n    \"cash_back\": \"1.5% on all purchases\",\n    \"bonus_offer\": \"Earn a $200 bonus after you spend $500 on purchases in the first 3 months from account opening\",\n    \"categories\": {\n      \"dining\": \"3% on dining at restaurants, including takeout and eligible delivery services\",\n      \"drugs\": \"3% at drugstores\",\n      \"travel\": \"5% on travel purchased through Chase Ultimate Rewards\"\n    }\n  },\n  \"annual_fee\": \"$0\",\n  \"foreign_transaction_fee\": \"$0\",\n  \"intro_offer\": \"0% intro APR on purchases for 15 months, then a variable APR of 19.24% - 27.49%\",\n  \"credit_score_needed\": \"Good to Excellent\",\n  \"other_benefits\": [\n    \"Purchase protection\",\n    \"Extended warranty\",\n    \"No foreign transaction fees\"\n  ]\n}\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```json\n{\n  \"title\": \"Which Chase Freedom Credit Card Should You Get?\",\n  \"author\": \"NerdWallet\",\n  \"date\": \"2023-10\",\n  \"summary\": \"A guide to choosing the best Chase Freedom credit card based on individual spending habits and rewards preferences.\",\n  \"key_points\": [\n    \"Overview of Chase Freedom credit card options.\",\n    \"Comparison of benefits and rewards among different Chase Freedom cards.\",\n    \"Tips for maximizing rewards based on spending categories.\",\n    \"Considerations for new applicants and existing Chase customers.\"\n  ]\n}\n```"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86Ek2sxb7SwR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}