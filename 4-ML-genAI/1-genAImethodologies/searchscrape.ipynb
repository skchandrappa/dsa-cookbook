{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# #### Pre Installs\n",
        "# import os\n",
        "# os.environ['openAIAPIKey'] = ''\n",
        "# os.environ['serpAPIKey'] = ''\n",
        "!pip install google-search-results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhApqbbXAupk",
        "outputId": "5ad30bb5-8bdf-418a-c98f-9d91b7291674"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2024.8.30)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32009 sha256=7bb8663db5768909401dd697993b670de3e8d49cb0d4bc5e4f0b4375c8d673b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "appConfig = {\n",
        "    \"NoOfLinks\" : 3\n",
        "}"
      ],
      "metadata": {
        "id": "3Jro5ntlS68W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "class getApiKeys:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def getApiKeys(self,apiKey):\n",
        "    return os.environ.get(apiKey)\n"
      ],
      "metadata": {
        "id": "OKQztJvr9_6h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import serpapi\n",
        "import json\n",
        "from serpapi import GoogleSearch # Add this import statement\n",
        "\n",
        "class searchForLinks:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def getSearchResults(self,searchWord):\n",
        "    searpApiKey = getApiKeys()\n",
        "    params = {\n",
        "      \"q\": searchWord,\n",
        "      \"hl\": \"en\",\n",
        "      \"gl\": \"us\",\n",
        "      \"google_domain\": \"google.com\",\n",
        "      \"api_key\": searpApiKey.getApiKeys('serpAPIKey')\n",
        "    }\n",
        "\n",
        "    search = GoogleSearch(params)\n",
        "    results = search.get_dict()\n",
        "    linksList = []\n",
        "\n",
        "    if 'organic_results' in results:\n",
        "        for res in results['organic_results']:\n",
        "            if 'link' in res:  # Check if 'link' key exists in each result dictionary\n",
        "                linksList.append(res['link'])\n",
        "    else:\n",
        "        print(\"No organic results found in the response.\")\n",
        "\n",
        "    return linksList[:appConfig['NoOfLinks']]"
      ],
      "metadata": {
        "id": "B9I34FupATv7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sfl = searchForLinks()\n",
        "sfl.getSearchResults('coffee')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBhuzSauPlr6",
        "outputId": "aca3124d-7098-4387-a59a-9188ab3ce211"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://en.wikipedia.org/wiki/Coffee',\n",
              " 'https://www.peets.com/',\n",
              " 'https://www.starbucks.com/']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "class webCrawler:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def crawlresults(self,srch):\n",
        "    sfl = searchForLinks()\n",
        "    linktocrawl = sfl.getSearchResults(srch)\n",
        "    print(linktocrawl)\n",
        "    crawlJson = {}\n",
        "    for links in linktocrawl:\n",
        "      crawlJson.update(self.crawl(links))\n",
        "\n",
        "    return crawlJson\n",
        "\n",
        "  def crawl(self, url: str):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    title = soup.title.string if soup.title else \"Title not found\"\n",
        "\n",
        "    if soup.body:\n",
        "      for irrelevant in soup.body([\"script\", \"style\",\"img\",\"input\"]):\n",
        "        irrelevant.decompose()\n",
        "      text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
        "    else:\n",
        "      text = \"\"\n",
        "\n",
        "    return {title : text}\n",
        "\n",
        "    # links = [link.get('href') for link in soup.find_all('a')]\n",
        "    # self.links = [link for link in links if link]"
      ],
      "metadata": {
        "id": "nRr7KRn8AeY7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crawl = webCrawler()\n",
        "crawl.crawlresults('Stock')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKUO9wpKAkgK",
        "outputId": "d7c6d843-0296-4f86-ed8c-5d662fe67ead"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No organic results found in the response.\n",
            "[]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86Ek2sxb7SwR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}